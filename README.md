# Local AI Desktop Assistant
A lightweight, offline desktop assistant powered by a local LLM (Ollama).

<p>GIF / Screenshot</p>

## üõ†Ô∏è Why I Built This
- I wanted an AI assistant I could actually trust and use daily.
- I don‚Äôt want cloud APIs, accounts, or background uploads.
- This project explores what a **local-first AI assistant** feels like on a personal computer.

## üß© Features
- üß† Local LLM via Ollama (offline, no API key)
- ü™ü Desktop-native window (always available, non-intrusive)
- üóÇÔ∏è Local memory (stored on device)
- üîí No cloud, no telemetry, no background upload

## ‚ñ∂Ô∏è Download & Install
- Windows / macOS installer
- Requires Ollama installed locally

## üß† How It Works (Short Version)
This app communicates with Ollama via a local HTTP API.
All conversations and memory stay on your machine.

> No data leaves your device unless you explicitly choose to.

## ‚ö†Ô∏è Notes
- This is a personal tool built for daily use.
- Model behavior depends on the model you choose in Ollama.

## üìú License
MIT



command:
/help /?
/python /py
/cmd
/powershell /ps /cli
/clear /cls
/open (google / github / youtube / chatgpt / gemini / yzu / gmail / outlook / calendar / translate / chess / ig / fb / yt / vscode)
/search (website name)
/math 
/chat
/chatgpt /gpt
/gemini
/shutdown (s / r) /sd (s / r)
/todo
/homework /hw
/note []
/chess
/dice
/card
