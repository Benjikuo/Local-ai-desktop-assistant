# Local AI Desktop Assistant
A lightweight, offline desktop assistant powered by a local LLM (Ollama).

<p>GIF / Screenshot</p>

## ğŸ› ï¸ Why I Built This
- I wanted an AI assistant I could actually trust and use daily.
- I donâ€™t want cloud APIs, accounts, or background uploads.
- This project explores what a **local-first AI assistant** feels like on a personal computer.

## ğŸ§© Features
- ğŸ§  Local LLM via Ollama (offline, no API key)
- ğŸªŸ Desktop-native window (always available, non-intrusive)
- ğŸ—‚ï¸ Local memory (stored on device)
- ğŸ”’ No cloud, no telemetry, no background upload

## â–¶ï¸ Download & Install
- Windows / macOS installer
- Requires Ollama installed locally

## ğŸ§  How It Works (Short Version)
This app communicates with Ollama via a local HTTP API.
All conversations and memory stay on your machine.

> No data leaves your device unless you explicitly choose to.

## âš ï¸ Notes
- This is a personal tool built for daily use.
- Model behavior depends on the model you choose in Ollama.

## ğŸ“œ License
MIT
